{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from LSsurf.unique_by_rows import unique_by_rows\n",
    "import pointCollection as pc\n",
    "import glob\n",
    "import re\n",
    "import h5py\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xover_dir='/home/besmith4//nobackup/xovers/rel005/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xovers(xover_dir, verbose=False, wildcard='*', r_limits=[0, 1.e7], delta_t_limit=2592000):\n",
    "    tiles=glob.glob(xover_dir+'/*.h5')\n",
    "    with h5py.File(tiles[0],'r') as h5f:\n",
    "        fields=[key for key in h5f['data_0'].keys()]\n",
    "    \n",
    "    D=[]\n",
    "    meta=[]\n",
    "    #X=[]\n",
    "    tile_re=re.compile('E(.*)_N(.*).h5')\n",
    "    count=0\n",
    "    tiles=glob.glob(xover_dir+'/'+wildcard+'.h5')\n",
    "    for tile in tiles:\n",
    "        m=tile_re.search(tile)\n",
    "        if m is not None:\n",
    "            r2=np.float(m.group(1))**2+np.float(m.group(2))**2\n",
    "            if (r2 < r_limits[0]**2) or (r2 > r_limits[1]**2):\n",
    "                continue\n",
    "        try:\n",
    "            this_D=[pc.data().from_h5(tile, field_dict={gr : fields}) for gr in ['data_0','data_1']]\n",
    "            if delta_t_limit is not None:\n",
    "                good=np.abs(this_D[1].delta_time[:,0]-this_D[0].delta_time[:,0]) < delta_t_limit\n",
    "                for Di in this_D:\n",
    "                    Di.index(good)\n",
    "                try:\n",
    "                    this_meta=pc.data().from_h5(tile, field_dict={None:['slope_x', 'slope_y','grounded']})\n",
    "                    this_meta.index(good)\n",
    "                except Exception:\n",
    "                    this_meta=None\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            if verbose:\n",
    "                print(\"failed to read \" + tile)\n",
    "            continue\n",
    "\n",
    "        D.append(this_D)\n",
    "        if this_meta is not None:\n",
    "            meta.append(this_meta)\n",
    "        #if np.mod(count, 100)==0:\n",
    "        #    print(f'{count} out of {len(tiles)}')\n",
    "    \n",
    "    v={}\n",
    "    for field in fields:\n",
    "        vi=[]\n",
    "        for Di in D:\n",
    "            vi.append(np.r_[[np.sum(getattr(Di[ii], field)*Di[ii].W, axis=1) for ii in [0, 1]]])\n",
    "        v[field]=np.concatenate(vi, axis=1).T\n",
    "    delta={field:np.diff(v[field], axis=1) for field in fields}\n",
    "    bar={field:np.mean(v[field], axis=1) for field in fields}\n",
    "    try:\n",
    "        meta=pc.data().from_list(meta)\n",
    "        meta={key:getattr(meta, key) for key in meta.fields}\n",
    "    except Exception:\n",
    "        pass\n",
    "    return v,  delta,  bar, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spread_vs_slope(delta0, rounded_slope_0, ind=None):\n",
    "    if ind is not None:\n",
    "        delta=delta0.copy_subset(ind)\n",
    "        rounded_slope=rounded_slope_0[ind]\n",
    "    else:\n",
    "        delta=delta0\n",
    "        rounded_slope=rounded_slope_0\n",
    "    _, slope_bins = unique_by_rows(rounded_slope, return_dict=True)\n",
    "    slope_vals=np.zeros(len(slope_bins.keys()))+np.NaN\n",
    "    spread=np.zeros_like(slope_vals)+np.NaN\n",
    "    N=np.zeros_like(slope_vals)+np.NaN\n",
    "    for ii, key in enumerate(slope_bins.keys()):\n",
    "        slope_vals[ii]=np.nanmedian(rounded_slope[slope_bins[key]])\n",
    "        dsub=delta.h_li[slope_bins[key]]\n",
    "        ind=np.ones_like(dsub, dtype=bool)\n",
    "        for k in np.arange(6):\n",
    "            sigma=np.sqrt(np.nanmean(dsub[ind]**2))\n",
    "            ind = np.abs(dsub) < 3*sigma\n",
    "        spread[ii]=sigma #ss.scoreatpercentile(np.abs(dsub), 0.68)\n",
    "        N[ii]=np.sum(ind)\n",
    "    return slope_vals, spread, N           \n",
    "\n",
    "def slope_regression(slope, spread, max_slope):\n",
    "    these= (slope < max_slope)\n",
    "    G=np.ones((np.sum(these), 2))\n",
    "    G[:,1]=slope[these]**2\n",
    "    m=np.linalg.solve(G.T.dot(G), G.T.dot(spread[these]**2))\n",
    "    V_model=G.dot(m)\n",
    "    V_model[V_model<0]=np.NaN\n",
    "    return [np.sqrt(m[0]), np.sqrt(m[1])], np.sqrt(V_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cycle( xover_dir, cycle):\n",
    "    #xover_dir='/home/besmith4/ben/xovers/Antarctica'+release+'/xovers/'+cycle+'/'\n",
    "    ATL06_fields=['BP','LR','W','cycle_number','rgt','h_li','h_li_sigma','x','y','spot', 'dh_fit_dx', 'dh_fit_dy']\n",
    "\n",
    "    v0,  delta0,  bar0, meta0 = read_xovers(xover_dir+'/'+cycle)\n",
    "    meta0['slope_mag']=np.abs(meta0['slope_x']+1j*meta0['slope_y'])\n",
    "    v=pc.data().from_dict(v0)\n",
    "    delta=pc.data().from_dict(delta0)\n",
    "    bar=pc.data().from_dict(bar0)\n",
    "    meta=pc.data().from_dict(meta0)\n",
    "\n",
    "    good=(meta.grounded==1) \n",
    "    print(np.mean(good))\n",
    "    v.index(good)\n",
    "    delta.index(good)\n",
    "    bar.index(good)\n",
    "    meta.index(good)\n",
    "    meta.assign({'slope':np.abs(meta.slope_x+1j*meta.slope_y)})\n",
    "\n",
    "    return v, delta, bar, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, d, b, m = read_cycle(xover_dir, 'c01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good=np.isfinite(vbar)\n",
    "good=np.sum(v.atl06_quality_summary, axis=1)==0\n",
    "v.index(good)\n",
    "d.index(good)\n",
    "b.index(good)\n",
    "m.index(good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bin the crossovers by the mean of the slope magnitude among all the segments in each crossover.  Then for each bin, calculate the median of the slope magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_slope=0.025\n",
    "slope_bin_width=0.00125\n",
    "\n",
    "v.assign({'slope_mag':np.abs(v.dh_fit_dx+1j*v.dh_fit_dy)})\n",
    "vbar=np.mean(v.slope_mag, axis=1)\n",
    "\n",
    "rounded_slope_m=np.round(m.slope/slope_bin_width)*slope_bin_width\n",
    "_, slope_bins = unique_by_rows(rounded_slope_m, return_dict=True)\n",
    "u_slopes=np.array(list(slope_bins.keys()))\n",
    "u_slopes=u_slopes[u_slopes<max_slope]\n",
    "m_stats=[]\n",
    "for ii in range(u_slopes.size):\n",
    "    m_stats+= [sps.scoreatpercentile(vbar[slope_bins[tuple([u_slopes[ii]])]], [16, 50, 84])]\n",
    "m_stats=np.r_[m_stats]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); plt.loglog(u_slopes, m_stats)\n",
    "plt.loglog(u_slopes, u_slopes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same as above, but calculate the slope in each bin based on the median of the DEM slopes from the xover metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbar=np.mean(v.slope_mag, axis=1)\n",
    "rounded_slope_v=np.round(vbar/slope_bin_width)*slope_bin_width\n",
    "_, slope_bins = unique_by_rows(rounded_slope_v, return_dict=True)\n",
    "u_slopes=np.array(list(slope_bins.keys()))\n",
    "u_slopes=u_slopes[u_slopes<max_slope]\n",
    "mv_stats=[]\n",
    "for ii in range(u_slopes.size):\n",
    "    mv_stats+= [sps.scoreatpercentile(m.slope[slope_bins[tuple([u_slopes[ii]])]], [16, 50, 84])]\n",
    "mv_stats=np.r_[mv_stats]\n",
    "plt.figure(); plt.loglog(u_slopes, mv_stats)\n",
    "plt.loglog(u_slopes, u_slopes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " same as above, but calculate the median slope based on the distribtion of all the segments making up the crossovers (rather than the mean slope as was done four cells up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_slope_v=np.round(v.slope_mag[:,1]/slope_bin_width)*slope_bin_width\n",
    "_, slope_bins = unique_by_rows(rounded_slope_v, return_dict=True)\n",
    "u_slopes=np.array(list(slope_bins.keys()))\n",
    "u_slopes=u_slopes[u_slopes<max_slope]\n",
    "mvv_stats=[]\n",
    "for ii in range(u_slopes.size):\n",
    "    mvv_stats+= [sps.scoreatpercentile(v.slope_mag[:,0][slope_bins[tuple([u_slopes[ii]])]], [16, 50, 84])]\n",
    "mvv_stats=np.r_[mvv_stats]\n",
    "plt.figure(); plt.loglog(u_slopes, mvv_stats, marker='.')\n",
    "plt.loglog(u_slopes, u_slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_slope=0.025\n",
    "slope_bin_width=0.00125\n",
    "slope_bin=np.round(meta.slope/slope_bin_width)*slope_bin_width\n",
    "\n",
    "slope_vals, spread, N=spread_vs_slope(delta, slope_bin)\n",
    "slope_fit=slope_regression(slope_vals[N>500], spread[N>500], max_slope)\n",
    "to_plot = (N>500) & (slope_vals <0.05)\n",
    "plt.figure(); plt.plot(slope_vals[to_plot], spread[to_plot],'.', label='data')\n",
    "plt.plot(slope_vals[slope_vals < max_slope], slope_fit[1],'r', label='%3.3f + %3.2f|slope|' % tuple(slope_fit[0]))\n",
    "plt.xlabel('slope magnitude')\n",
    "plt.ylabel('xover difference spread')\n",
    "plt.legend()\n",
    "plt.title('all spots, cycle 09')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_fit[0]\n",
    "'%3.3f + %3.2f|slope|' % tuple(slope_fit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits={}\n",
    "\n",
    "for spot0 in range(1,7):\n",
    "    for spot1 in range(1, spot0+1):\n",
    "        #print((spot0, spot1))\n",
    "        these = ((v.spot[:,0]==spot0) & (v.spot[:,1]==spot1)) | ((v.spot[:,0]==spot1) & (v.spot[:,1]==spot0))\n",
    "        slope_vals, spread, N=spread_vs_slope(delta[these], slope_bin[these])\n",
    "        fits[(spot0, spot1)]=slope_regression(slope_vals[N>100], spread[N>100], max_slope)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots=np.c_[list(fits.keys())]\n",
    "sigma_0=np.c_[[fits[tuple(spots[row,:])][0] for row in range(spots.shape[0])]]\n",
    "sigma_x=np.c_[[fits[tuple(spots[row,:])][1] for row in range(spots.shape[0])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); plt.scatter(spots[:,0], spots[:,1], 30, c=sigma_0, cmap='jet', vmin=0.045, vmax=0.1); \n",
    "plt.colorbar(label='flat-surface error, m')\n",
    "plt.title('cycle 09')\n",
    "plt.ylabel('spot')\n",
    "plt.figure(); plt.scatter(spots[:,0], spots[:,1], 30, c=sigma_x, cmap='jet', vmin=2.5, vmax=8); \n",
    "plt.colorbar(label='pointing error, m')\n",
    "plt.ylabel('spot')\n",
    "plt.xlabel('spot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code to find per-rgt errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error analysis to pick out the worst RGTs:\n",
    "DD={}\n",
    "sign=[1, -1]\n",
    "dh_dict={}\n",
    "y_atc_dict={}\n",
    "spot_dict={}\n",
    "dt_dict={}\n",
    "for col in [0, 1]:\n",
    "    # loop over combinations of cycles and tracks in each column\n",
    "    u_ct, D_ct = unique_by_rows(np.c_[np.round(v.rgt[:, col]).astype(int), np.round(v.cycle_number[:,col]).astype(int)], return_dict=True)\n",
    "    for ct in u_ct:\n",
    "        key=tuple(ct)\n",
    "        if key not in dh_dict:\n",
    "            dh_dict[key]=[]\n",
    "            y_atc_dict[key]=[]\n",
    "            spot_dict[key]=[]\n",
    "            dt_dict[key]=[]\n",
    "        these=D_ct[key]\n",
    "        good=(bar.atl06_quality_summary[these]<0.01) & (meta.slope_mag[these]<0.02)\n",
    "        if np.any(good):\n",
    "            dh_dict[key].append(delta.h_li[these[good]]*sign[col])\n",
    "            y_atc_dict[key].append(v.y_atc[these,col][good])\n",
    "            spot_dict[key].append(v.spot[these,col][good])\n",
    "            dt_dict[key].append(v.delta_time[these,col][good])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dict holding per-rgt ct_stats_d\n",
    "\n",
    "ct_stats_d={}\n",
    "for key in dh_dict:\n",
    "    if len(dh_dict[key]) ==0:\n",
    "        continue\n",
    "    dh=np.concatenate([item.ravel() for item in dh_dict[key]])\n",
    "    spot=np.concatenate([item.ravel() for item in spot_dict[key]])\n",
    "    y=np.concatenate([item.ravel() for item in y_atc_dict[key]])\n",
    "    tt=np.concatenate([item.ravel() for item in dt_dict[key]])\n",
    "    ct_stats_d[key]={\n",
    "        'med':np.nanmedian(dh),\n",
    "        'count':dh.size,\n",
    "        'mad':np.nanmedian(np.abs(dh-np.nanmedian(dh))),\n",
    "       'good_count':np.nansum(np.abs(dh)<1).astype(np.float),\n",
    "        'y_atc':np.nanmean(y[np.abs(spot-3.5)<0.6]),\n",
    "        't0':np.nanmin(tt),\n",
    "        't1':np.nanmax(tt)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the ct_stats_dictionary\n",
    "\n",
    "keys=list(ct_stats_d.keys())\n",
    "ct_stats={\n",
    "    'med':np.array([ct_stats_d[key]['med'] for key in keys]),\n",
    "    'mad':np.array([ct_stats_d[key]['mad'] for key in keys]),\n",
    "    'count':np.array([ct_stats_d[key]['count'] for key in keys]),\n",
    "    'good_count':np.array([ct_stats_d[key]['good_count'] for key in keys]),\n",
    "    'rgt':np.array([key[0] for key in keys]),\n",
    "    'cycle':np.array([key[1] for key in keys]),\n",
    "    'y':np.array([ct_stats_d[key]['y_atc'] for key in keys]),\n",
    "    't0':np.array([ct_stats_d[key]['t0'] for key in keys]),\n",
    "    't1':np.array([ct_stats_d[key]['t1'] for key in keys])\n",
    "}\n",
    "\n",
    "for field in ['med', 'mad', 'good_count']:\n",
    "    ct_stats[field][ct_stats['count']<100]=np.NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           \n",
    "fig=plt.figure();\n",
    "ax=fig.add_subplot(121)\n",
    "ax.hist(np.log10(ct_stats['mad']), 25)\n",
    "ax.set_xlabel('log10(MAD, m)')\n",
    "ax=fig.add_subplot(122)\n",
    "ax.hist(ct_stats['med'], 25)\n",
    "ax.set_xlabel('med, m')\n",
    "\n",
    "fig=plt.figure();\n",
    "ax=fig.add_subplot(111)\n",
    "ax.plot(ct_stats['rgt'], ct_stats['med'], 'k', label='medians')\n",
    "ax.plot(ct_stats['rgt'], ct_stats['med']-ct_stats['mad'], 'r--', label='medians +- mads')\n",
    "ax.plot(ct_stats['rgt'], ct_stats['med']+ct_stats['mad'] , 'r--')\n",
    "ax.legend()\n",
    "ax.set_xlabel('rgt')\n",
    "ax.set_ylabel('xover differece, m')\n",
    "\n",
    "print('found rgts with medians greater than 8 cm:')\n",
    "print(ct_stats['rgt'][np.abs(ct_stats['med'])>0.08])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /Volumes/ice2/ben/scf/Uyuni/003/xovers/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vu, du, bu, mu = read_xovers('/Volumes/ice2/ben/scf/Uyuni/003/xovers/', r_limits=[0, 1.e12], delta_t_limit=2592000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vu=pc.data().from_dict(vu)\n",
    "du=pc.data().from_dict(du)\n",
    "bu=pc.data().from_dict(bu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); \n",
    "ii=np.argsort(np.abs(du.h_li.ravel()))\n",
    "plt.scatter(bu.longitude[ii], bu.latitude[ii], c=du.h_li[ii], clim=[-1, 1], cmap='Spectral')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IS2",
   "language": "python",
   "name": "is2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
